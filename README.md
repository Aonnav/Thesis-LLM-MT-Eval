# Thesis-LLM-MT-Eval
The code used for my Master Thesis "Massively Lost in Translation: A comparison of Large Language Models on the Task of Translation"
The project was run through Google Colab. The .ipynb is thus the original file used. The .py file is automatically generated by Google Colab.

DSECT:
The Dutch Parallel Corpus is owned by Taalunie and is accessible at https://taalmaterialen.ivdnt.org/download/tstc-dutch-parallel-corpus-niet-commercieel/ through a (free) account. The PoetryTranslationEMNLP2021 is available at https://github.com/tuhinjubcse/PoetryTranslationEMNLP2021.

With respect to the code, multiple sources have been consulted. The code for mBART has been adapted from the publically available mBART Huggingface page, which is available here: https://huggingface.co/docs/transformers/model_doc/mbart. The code for Jurassic-2 has been adapted from the AI21 Playground, which is available after creating a (free) account at: https://studio.ai21.com/playground/. The code for GPT3 has been adapted from the OpenAI Playground, which is also freely available after creating a (free) account at: https://platform.openai.com/playground. The code for COMET has been adapted from the Huggingface page of COMET, available at:https://huggingface.co/Unbabel/wmt22-comet-da. The code for the lexical richness evaluation has been adapted from the publically available GitHub page of dr. Dimitar Shterionov, available https://github.com/dimitarsh1/BiasMT/blob/main/scripts/diversity/biasmt_metrics.py. For the classification, the code that can be found at https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568 and https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6 has been consulted and adapted.
